{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kASxwnKHsDa","outputId":"79f63141-cb35-41d6-a729-4a9d51f7f804","executionInfo":{"status":"ok","timestamp":1763203240559,"user_tz":-210,"elapsed":92958,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXKdeZfpG-Sx","outputId":"3e3bbed6-3d10-462c-fd6d-b0a2036b5a29","executionInfo":{"status":"ok","timestamp":1763203242096,"user_tz":-210,"elapsed":1546,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Crane'...\n","remote: Enumerating objects: 358, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 358 (delta 7), reused 14 (delta 4), pack-reused 333 (from 1)\u001b[K\n","Receiving objects: 100% (358/358), 9.14 MiB | 16.48 MiB/s, done.\n","Resolving deltas: 100% (156/156), done.\n"]}],"source":["!git clone https://github.com/YasinSharifbeigy/Crane.git"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-P430pU-KAgy","outputId":"bffc2e4a-f7cc-4128-d8b8-52806690a60f","executionInfo":{"status":"ok","timestamp":1763203242101,"user_tz":-210,"elapsed":5,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Crane\n"]}],"source":["%cd Crane"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsdnuFhprn65","outputId":"595bb202-57bb-470b-a843-dc1ec72a9b18","executionInfo":{"status":"ok","timestamp":1763203257570,"user_tz":-210,"elapsed":15470,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting humanhash3\n","  Downloading humanhash3-0.0.6.tar.gz (5.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: humanhash3\n","  Building wheel for humanhash3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for humanhash3: filename=humanhash3-0.0.6-py3-none-any.whl size=5739 sha256=37a1869326a5baf5e72aad226a11b4a79cb3efc41d4e3f4bea914b2043317931\n","  Stored in directory: /root/.cache/pip/wheels/24/9c/a6/944e159dadbcc308d4b0b12e536503849bf4b9bbcc4036fae6\n","Successfully built humanhash3\n","Installing collected packages: humanhash3\n","Successfully installed humanhash3-0.0.6\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n","Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n"]}],"source":["!pip install humanhash3\n","!pip install torchmetrics\n","!pip install ftfy"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjkJ-AEITi4m","outputId":"014b8e93-5613-4f8d-aaff-32718fbfd960","executionInfo":{"status":"ok","timestamp":1763203257681,"user_tz":-210,"elapsed":103,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["assets\t     environment.yml  models\t    runtime.sh\t      test.py\tutils\n","checkpoints  __init__.py      README.md     segment_anything  test.sh\n","dataset      LICENSE\t      reproduce.sh  setup.sh\t      train.py\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"diJSye5uLPCs","executionInfo":{"status":"ok","timestamp":1763203257783,"user_tz":-210,"elapsed":91,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[],"source":["!mkdir '.cache'"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-785zjWxNQe7","executionInfo":{"status":"ok","timestamp":1763203257960,"user_tz":-210,"elapsed":170,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[],"source":["!mkdir '.cache/sam'"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VsY5-gPPNhFQ","executionInfo":{"status":"ok","timestamp":1763203257961,"user_tz":-210,"elapsed":19,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[],"source":["\n","# !wget -O .cache/sam/sam_vit_b_01ec64.pth \\\n","# https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HP9uqjYEUicr","executionInfo":{"status":"ok","timestamp":1763203257968,"user_tz":-210,"elapsed":24,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[],"source":["DATASETS_ROOT = '/content/drive/MyDrive/Colposcopy/Data/Patient records'"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eEaIerNvN_sV","executionInfo":{"status":"ok","timestamp":1763203257969,"user_tz":-210,"elapsed":16,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"outputs":[],"source":["# !bash reproduce.sh Colpo_model 0 1"]},{"cell_type":"code","source":["# ==========================================================\n","# üì¶ 1. Imports\n","# ==========================================================\n","import os, sys, subprocess\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix\n","\n","# your project-specific imports\n","import models\n","from models import Crane\n","from models.prompt_ensemble import PromptLearner\n","from dataset.dataset import Dataset\n","from __init__ import DATASETS_ROOT\n","from utils.transform import get_transform\n","from utils.logger import get_logger\n","from utils.similarity import calc_similarity_logits\n","from utils import (\n","    setup_seed,\n","    seed_worker,\n","    turn_gradient_off,\n","    str2bool,\n",")\n","from utils.loss import FocalLoss\n"],"metadata":{"id":"j4WXj1ZmRBJw","executionInfo":{"status":"ok","timestamp":1763203278269,"user_tz":-210,"elapsed":20310,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# ‚öôÔ∏è 2. Argument setup (for interactive control)\n","# ==========================================================\n","class Args:\n","    datasets_root_dir = f\"{DATASETS_ROOT}\"\n","    dataset = [\"Cropped Folder\"]\n","    train_data_path = [f\"{DATASETS_ROOT}/{ds}/\" for ds in dataset]\n","    save_path = f\"./checkpoints/trained_on_{dataset[0]}_Colpo_model_crane\"\n","    model_name = \"Colpo_model_crane\"\n","    type = \"train\"\n","    seed = 111\n","    save_freq = 1\n","    device = 0\n","    epoch = 2\n","    learning_rate = 1e-3\n","    batch_size = 8\n","    aug_rate = 0.0\n","    k_shot = 0\n","    portion = 1\n","    image_size = 518\n","    features_list = [24]\n","    interpolation = \"nearest\"\n","    depth = 9\n","    n_ctx = 12\n","    t_n_ctx = 4\n","    train_with_img_cls_prob = 1\n","    train_with_img_cls_type = \"pad_suffix\"\n","    dino_model = \"dinov2\"\n","    both_eattn_dattn = True\n","    use_scorebase_pooling = True\n","    attn_type = \"qq+kk+vv\"\n","    why = \"Jupyter experiment\"\n","\n","args = Args()\n","setup_seed(args.seed)\n"],"metadata":{"id":"ns1AQK2aBmrc","executionInfo":{"status":"ok","timestamp":1763203278271,"user_tz":-210,"elapsed":1,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# üß† 3. Load Dataset and Model\n","# ==========================================================\n","preprocess, target_transform = get_transform(args)\n","train_data = Dataset(\n","    roots=args.train_data_path,\n","    transform=preprocess,\n","    target_transform=target_transform,\n","    dataset_name=args.dataset,\n","    kwargs=args\n",")\n","\n","train_loader = DataLoader(\n","    train_data,\n","    batch_size=args.batch_size,\n","    shuffle=True,\n","    num_workers=8,\n","    pin_memory=True,\n","    prefetch_factor=2,\n","    generator=torch.Generator().manual_seed(args.seed),\n","    worker_init_fn=seed_worker\n",")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"‚úÖ Device: {device}, Dataset length: {len(train_data)}\")\n","\n","# -------------------- Load Model --------------------\n","crane_parameters = {\n","    \"Prompt_length\": args.n_ctx,\n","    \"learnabel_text_embedding_depth\": args.depth,\n","    \"learnabel_text_embedding_length\": args.t_n_ctx,\n","    \"others\": args\n","}\n","model, _ = models.load(\"ViT-L/14@336px\", device=device, design_details=crane_parameters)\n","model = turn_gradient_off(model)\n","model.visual.replace_with_EAttn(to_layer=20, type=args.attn_type)\n","if args.dino_model != \"none\":\n","    model.use_DAttn(args.dino_model)\n","\n","prompt_learner = PromptLearner(model.to(\"cpu\"), crane_parameters)\n","sbp = Crane.ScoreBasePooling()\n","model.to(device)\n","prompt_learner.to(device)\n","\n","optimizer = torch.optim.Adam(prompt_learner.parameters(), lr=args.learning_rate, betas=(0.6, 0.999))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uM8pKYvcRSj0","executionInfo":{"status":"ok","timestamp":1763203351919,"user_tz":-210,"elapsed":73647,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}},"outputId":"fa7103c0-4cbd-431d-f421-bf6a248ab14b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["number of samples: 318\n","‚úÖ Device: cuda, Dataset length: 318\n","name ViT-L/14@336px\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 934M/934M [00:51<00:00, 18.1MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Turning off gradients in both the image and the text encoder\n","Parameters to be updated: set()\n","Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"stream","name":"stderr","text":["/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n","  warnings.warn(\"xFormers is not available (SwiGLU)\")\n","/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n","  warnings.warn(\"xFormers is not available (Attention)\")\n","/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n","  warnings.warn(\"xFormers is not available (Block)\")\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_reg4_pretrain.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:01<00:00, 336MB/s]\n"]}]},{"cell_type":"code","source":["class FocalLoss(torch.nn.Module):\n","    def __init__(self, weight=None, gamma=2.0, reduction=\"mean\"):\n","        super(FocalLoss, self).__init__()\n","        self.weight = weight\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, logits, target):\n","        # target shape: [B] or [B,1]\n","        if target.dim() == 1:\n","            target = target.unsqueeze(1)\n","\n","        target = target.view(-1)  # shape [B]\n","        logits = logits.view(-1, logits.size(-1))  # shape [B, num_classes]\n","\n","        log_probs = F.log_softmax(logits, dim=-1)\n","        probs = torch.exp(log_probs)\n","\n","        # gather logprobs of the correct class\n","        log_pt = log_probs[torch.arange(len(target)), target]\n","        pt = probs[torch.arange(len(target)), target]\n","\n","        # apply class weights if provided\n","        if self.weight is not None:\n","            at = self.weight[target]\n","        else:\n","            at = 1.0\n","\n","        loss = -at * ((1 - pt) ** self.gamma) * log_pt\n","\n","        if self.reduction == \"mean\":\n","            return loss.mean()\n","        elif self.reduction == \"sum\":\n","            return loss.sum()\n","        return loss\n"],"metadata":{"id":"QSVVRSmoIleb","executionInfo":{"status":"ok","timestamp":1763203769575,"user_tz":-210,"elapsed":59,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0], device=device))\n","criterion = FocalLoss(weight=torch.tensor([1, 100.0], device=device), gamma=2.0)\n","# criterion = FocalLoss(gamma=2.0)\n"],"metadata":{"id":"kQIZR9nug-9K","executionInfo":{"status":"ok","timestamp":1763207298598,"user_tz":-210,"elapsed":8,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# üöÄ 4. Training Loop (with confusion matrix logging)\n","# ==========================================================\n","log_interval = 10  # show confusion matrix every N batches\n","logger = get_logger(args.save_path)\n","\n","model.eval()\n","prompt_learner.train()\n","\n","for epoch in tqdm(range(args.epoch), desc=\"Epochs\"):\n","    loss_list = []\n","    all_preds, all_labels = [], []\n","\n","    for batch_idx, items in enumerate(train_loader):\n","        label = items[\"anomaly\"].to(device)\n","        image = items[\"img\"].to(device)\n","\n","        # Encode\n","        image_features, patch_features = model.encode_image(image, args.features_list, self_cor_attn_layers=20)\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        # Text prompts\n","        prompts, tokenized_prompts, compound_prompts_text, is_train_with_img_cls = prompt_learner(img_emb=image_features)\n","        if is_train_with_img_cls:\n","            text_features_nrm = model.encode_text_learn(prompts[0], tokenized_prompts[0], compound_prompts_text)\n","            text_features_anm = model.encode_text_learn(prompts[1], tokenized_prompts[1], compound_prompts_text)\n","            text_features = torch.stack([text_features_nrm, text_features_anm], dim=1)\n","        else:\n","            text_features = model.encode_text_learn(prompts, tokenized_prompts, compound_prompts_text).unsqueeze(dim=0)\n","        text_features = F.normalize(text_features, dim=-1).float()\n","\n","        # Score-base pooling (optional)\n","        if args.use_scorebase_pooling:\n","            sms = [calc_similarity_logits(pf, text_features, temp=0.07) for pf in patch_features]\n","            patch_features = torch.stack(patch_features, dim=0)  # ü©π FIX\n","            clustered_feature = sbp.forward(patch_features, sms)\n","            image_features = 0.5 * clustered_feature + 0.5 * image_features\n","            image_features = F.normalize(image_features, dim=1)\n","\n","\n","        # Classification\n","        image_logits = calc_similarity_logits(image_features, text_features, temp=0.01)\n","        loss = criterion(image_logits, label.long())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Record loss and predictions\n","        loss_list.append(loss.item())\n","        preds = image_logits.argmax(dim=-1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(label.cpu().numpy())\n","\n","        # Show confusion matrix every N batches\n","        if (batch_idx + 1) % log_interval == 0:\n","            cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n","            acc = np.trace(cm) / np.sum(cm)\n","            print(f\"\\nEpoch {epoch+1}, Batch {batch_idx+1}: Loss={np.mean(loss_list):.4f}, Acc={acc:.3f}\")\n","            print(cm)\n","            all_preds, all_labels = [], []  # reset window\n","\n","    # End of epoch\n","    avg_loss = np.mean(loss_list)\n","    logger.info(f\"Epoch [{epoch+1}/{args.epoch}] - loss: {avg_loss:.4f}\")\n","\n","    # Save model checkpoint\n","    if (epoch + 1) % args.save_freq == 0:\n","        os.makedirs(args.save_path, exist_ok=True)\n","        ckpt_path = os.path.join(args.save_path, f\"epoch_{epoch+1}.pth\")\n","        torch.save({\"prompt_learner\": prompt_learner.state_dict()}, ckpt_path)\n","        print(f\"‚úÖ Saved: {ckpt_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CM5qXpgRfi-","executionInfo":{"status":"ok","timestamp":1763207834844,"user_tz":-210,"elapsed":535641,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}},"outputId":"33a8ff4d-2b02-43f0-e964-328a2f022371"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpochs:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1, Batch 10: Loss=0.7123, Acc=0.600\n","[[ 0 32]\n"," [ 0 48]]\n","\n","Epoch 1, Batch 20: Loss=0.7224, Acc=0.650\n","[[ 0 28]\n"," [ 0 52]]\n","\n","Epoch 1, Batch 30: Loss=0.7650, Acc=0.588\n","[[ 0 33]\n"," [ 0 47]]\n","\n","Epoch 1, Batch 40: Loss=0.7211, Acc=0.692\n","[[ 0 24]\n"," [ 0 54]]\n"]},{"output_type":"stream","name":"stderr","text":["25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","25-11-15 11:52:47.628 - INFO: Epoch [1/2] - loss: 0.7211\n","Epochs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [04:27<04:27, 267.85s/it]"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Saved: ./checkpoints/trained_on_Cropped Folder_Colpo_model_crane/epoch_1.pth\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2, Batch 10: Loss=0.7362, Acc=0.625\n","[[ 0 30]\n"," [ 0 50]]\n","\n","Epoch 2, Batch 20: Loss=0.6987, Acc=0.662\n","[[ 0 27]\n"," [ 0 53]]\n","\n","Epoch 2, Batch 30: Loss=0.7151, Acc=0.613\n","[[ 0 31]\n"," [ 0 49]]\n","\n","Epoch 2, Batch 40: Loss=0.6981, Acc=0.628\n","[[ 0 29]\n"," [ 0 49]]\n"]},{"output_type":"stream","name":"stderr","text":["25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","25-11-15 11:57:15.399 - INFO: Epoch [2/2] - loss: 0.6981\n","Epochs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [08:55<00:00, 267.81s/it]"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Saved: ./checkpoints/trained_on_Cropped Folder_Colpo_model_crane/epoch_2.pth\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["!bash reproduce.sh Colpo_model 0 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZTYOh3mTRYX","executionInfo":{"status":"ok","timestamp":1763207952082,"user_tz":-210,"elapsed":117231,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}},"outputId":"d31de49b-a0b1-458f-ffae-6082f00d7da2"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["The name for base version (Crane) is: Colpo_model_crane\n","Testing on dataset from: ['/content/drive/MyDrive/Colposcopy/Data/Patient records/Cropped Folder/']\n","Results will be saved to: \u001b[32m./results/trained_on_Cropped Folder_Colpo_model_crane/test_on_Cropped Folder/salami-nuts\u001b[0m\n","Warning: The file ./results/trained_on_Cropped Folder_Colpo_model_crane/test_on_Cropped Folder/salami-nuts/args.txt already exists and will be overwritten.\n","name ViT-L/14@336px\n","Turning off gradients in both the image and the text encoder\n","Parameters to be updated: set()\n","number of samples: 106\n","process_dataset, Batch size: 8\n","/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Processing test samples: 100% 14/14 [01:26<00:00,  6.17s/it]\n","calculating metrics for coloposcopy\n","Best threshold=0.5350, F1=0.8235\n","Confusion Matrix (labels=[0,1], threshold=0.53):\n","[[TN=   6, FP=  27],\n"," [FN=   3, TP=  70]]\n","Best threshold=0.5350, F1=0.8235\n","Confusion Matrix (labels=[0,1], threshold=0.53):\n","[[TN=   6, FP=  27],\n"," [FN=   3, TP=  70]]\n","Best threshold=0.5350, F1=0.8235\n","Confusion Matrix (labels=[0,1], threshold=0.53):\n","[[TN=   6, FP=  27],\n"," [FN=   3, TP=  70]]\n","only one class present, can not calculate pixel metrics\n","only one class present, can not calculate pixel metrics\n","only one class present, can not calculate pixel metrics\n","Cropped Folder\n","25-11-15 11:59:10.261 - INFO: \n","| objects     |   pixel_auroc |   pixel_aupro |   pixel_f1 |   image_auroc |   image_ap |   image_f1 |\n","|:------------|--------------:|--------------:|-----------:|--------------:|-----------:|-----------:|\n","| coloposcopy |         0.000 |         0.000 |      0.000 |         0.698 |      0.848 |      0.824 |\n","| mean        |         0.000 |         0.000 |      0.000 |         0.698 |      0.848 |      0.824 |\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DvnlhUuOYqn0","executionInfo":{"status":"aborted","timestamp":1763203374342,"user_tz":-210,"elapsed":227021,"user":{"displayName":"Yasin Sharif beigy","userId":"11366778608532892598"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}